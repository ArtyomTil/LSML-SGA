from pyspark.sql import SparkSession
from pyspark.sql.functions import col, concat_ws, collect_list

import findspark
findspark.init()

# Initialize Spark session
spark = SparkSession.builder.appName("ClickstreamAnalysisSQL").getOrCreate()

# Read the data
clickstream_df = spark.read.csv('clickstream.csv', sep='\t', header=True, inferSchema=True)

# Register the DataFrame as a SQL temporary view
clickstream_df.createOrReplaceTempView("clickstream")

# SQL query to process the data
query = """
WITH Flagged AS (
    SELECT 
        *, 
        MAX(CASE WHEN event_type LIKE '%error%' THEN 1 ELSE 0 END) OVER (PARTITION BY user_id,
                                                                                      session_id
                                                                         ORDER BY timestamp
                                                                         ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW
                                                                         ) AS error_flag
    FROM clickstream
)
SELECT route, COUNT(*) as count
FROM (
    SELECT 
        user_id, 
        session_id,
        CONCAT_WS('-', COLLECT_LIST(event_page) OVER (PARTITION BY user_id, 
                                                                   session_id
                                                     ORDER BY timestamp 
                                                     ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)
                                                     ) AS route
    FROM Flagged
    WHERE error_flag = 0
) GROUP BY route
ORDER BY count DESC
LIMIT 30
"""

# Execute the query and show results
top_routes_sql = spark.sql(query)
top_routes_sql.show(30, truncate=False)

# Create a file with results
top_routes_sql.coalesce(1).write.format('csv').option("header", "true").save("resultSQL.csv")
# Stop the Spark session
spark.stop()

# Create a file with results
top_routes_sql.coalesce(1).write.format('csv').option("header", "true").save("resultSQL.csv")
# Stop the Spark session
spark.stop()
