{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4df32f5",
   "metadata": {},
   "source": [
    "Given map-reduce sequence of tasks, what would be the algorithm to convert it into Spark, can one improve it in speed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97bb423",
   "metadata": {},
   "source": [
    "I will illustrate the process of converting a map-reduce sequence to Spark and optimizing it with a simple example. Let's assume a map-reduce task that processes the text of \"Alice's Adventures in Wonderland\" to calculate the frequency of each word. In a common map-reduce approach, this would involve:\n",
    "1)\tMap Phase: Tokenizing the text into words and mapping each word to a count of 1.\n",
    "\n",
    "2)\tReduce Phase: Aggregating these counts by word to get the total count of each word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec1c3e7",
   "metadata": {},
   "source": [
    "Converting to Spark\n",
    "\n",
    "Below, we'll convert this map-reduce logic into a Spark program using PySpark.\n",
    "\n",
    "Step 1: Setup Spark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b49e60f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WordCount\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cc9f41",
   "metadata": {},
   "source": [
    "Step 2: Load and Process the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "405fe689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text file into an RDD\n",
    "text_file = spark.sparkContext.textFile(\"alice.txt\")\n",
    "\n",
    "# Map Phase: Tokenize the text into words and map each to a count of 1\n",
    "words = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "                 .filter(lambda word: word != \"\")  # Remove empty tokens\n",
    "word_pairs = words.map(lambda word: (word, 1))\n",
    "\n",
    "# Reduce Phase: Aggregate counts by word\n",
    "word_counts = word_pairs.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Collect and print the word counts\n",
    "for word, count in word_counts.take(20):\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fa9856",
   "metadata": {},
   "source": [
    "How to improve it\n",
    "\n",
    "In-Memory Computing: If further processing is required on word_counts, it could be beneficial to cache the RDD using word_counts.cache() before the action that collects or outputs the data.\n",
    "\n",
    "DataFrames Over RDDs: Converting the RDD to a DataFrame and using Spark SQL for aggregation might offer performance benefits through optimized execution plans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebe0d255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert RDD to DataFrame\n",
    "word_counts_df = word_counts.toDF([\"word\", \"count\"])\n",
    "\n",
    "# Use Spark SQL for potentially optimized execution\n",
    "word_counts_df.createOrReplaceTempView(\"word_counts\")\n",
    "spark.sql(\"SELECT word, count FROM word_counts ORDER BY count DESC LIMIT 20\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbd7059",
   "metadata": {},
   "source": [
    "Avoiding Joins\n",
    "\n",
    "In this example, joins aren't directly applicable.\n",
    "However, in theory, if we had to enrich word counts with additional information from another dataset, we could potentially use broadcast variables to avoid expensive shuffle operations associated with joins. For example, if we had a small dataset mapping words to their parts of speech, we could broadcast this dataset and use it during the map phase to annotate each word with its part of speech before counting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34509a0",
   "metadata": {},
   "source": [
    "When avoiding Joins is faster\n",
    "\n",
    "Avoiding joins is typically faster when you can broadcast a small dataset to all nodes, allowing each node to locally combine data without the need for shuffling large amounts of data across the network. This approach is beneficial for lookup operations or when enriching a large dataset with additional attributes from a smaller dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
